{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from net.layer import BasicCatDim1, BasicIdentity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and parse_graph function\n",
    "You can also define your own parse_graph function when the automatic parse_graph fails or you want to customize the granularity of your computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.conv_1 = nn.Conv2d(64, 32, 1, stride=2, padding=0, bias=False)\n",
    "        self.conv_2 = nn.Conv2d(64, 32, 1, stride=2, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        x1 = self.conv_1(x)\n",
    "        x2 = self.conv_2(x)\n",
    "        out = torch.cat([x1, x2], dim=1)\n",
    "        out = self.bn(out)\n",
    "        return out\n",
    "\n",
    "    def parse_graph(self, x):\n",
    "        # parse_graph forwards the network and create nodes and edges for the computation graph\n",
    "        # Each node is a tensor and has field cost=tensor.numel(), each edge is an operation and has field cost=0, module=op\n",
    "        # You may want to combine nn.ReLU(inplace=True) with previous operation into 1 op so that the computation graph \n",
    "        # reflects the actual memory usage\n",
    "        \n",
    "        G = nx.MultiDiGraph()\n",
    "        source = 0\n",
    "        vertex_id = 0\n",
    "        G.add_node(vertex_id, cost=x.numel())\n",
    "        \n",
    "        # inplace=False for relu so not combining relu and conv2d into 1 op\n",
    "        op = self.relu\n",
    "        x = op(x)\n",
    "        G.add_node(vertex_id + 1, cost=x.numel())\n",
    "        G.add_edge(vertex_id, vertex_id + 1, cost=0, module=op)\n",
    "        vertex_id += 1\n",
    "        input_id = vertex_id\n",
    "\n",
    "\n",
    "        op1 = self.conv_1\n",
    "        x1 = op1(x)\n",
    "        G.add_node(vertex_id + 1, cost=x1.numel())\n",
    "        G.add_edge(input_id, vertex_id + 1, cost=0, module=op1)\n",
    "        vertex_id += 1\n",
    "        x1_id = vertex_id\n",
    "\n",
    "\n",
    "        op2 = self.conv_2\n",
    "        x2 = op2(x)\n",
    "        G.add_node(vertex_id + 1, cost=x2.numel())\n",
    "        G.add_edge(input_id, vertex_id + 1, cost=0, module=op2)\n",
    "        vertex_id += 1\n",
    "        x2_id = vertex_id\n",
    "\n",
    "        # for an op with multiple input tensors, we define the op as a transition op, to transition multiple input tensor into 1 output tensor\n",
    "        # for example x = torch.cat([x1, x2]), we need to first add node x to the computation graph\n",
    "        # and put torch.cat operation as a transition operation into node x, and define the tranisition order as [x1_id, x2_id] for node x. \n",
    "        # Then we need to add the edge x1->x, x2->x into the computation graph. For these two edges, we just need to put place holder \n",
    "        # operation BasicIdentity to it. BasicIdentity does nothing and directly returns the tensor. \n",
    "        \n",
    "        op = BasicCatDim1()\n",
    "        identity = BasicIdentity()\n",
    "        x = op([x1, x2])\n",
    "        G.add_node(vertex_id + 1, cost=x.numel(), transition=op)\n",
    "        G.nodes[vertex_id + 1]['transition_input_order'] = []\n",
    "        for id in [x1_id, x2_id]:\n",
    "            edge_id = G.add_edge(id, vertex_id + 1, cost=0, module=identity)\n",
    "            G.nodes[vertex_id + 1]['transition_input_order'].append((id, edge_id))\n",
    "        vertex_id += 1\n",
    "\n",
    "        op = self.bn\n",
    "        x = op(x)\n",
    "        G.add_node(vertex_id + 1, cost=x.numel())\n",
    "        G.add_edge(vertex_id, vertex_id + 1, cost=0, module=op)\n",
    "        vertex_id += 1\n",
    "        \n",
    "        target = vertex_id\n",
    "        \n",
    "        # finally we returns the computation graph, source and target node key of the computation graph\n",
    "        return G, source, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and backward check of the parsed graph\n",
    "If our forward function contains random operation, such as dropout, the forward and backward check will fail as the rng status has changed. We may want to disable the random operation when doing forward backward check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed graph forward check passed\n",
      "Parsed graph backward check passed\n"
     ]
    }
   ],
   "source": [
    "from graph import Segment, set_segment_training\n",
    "\n",
    "def forward_check(net, parsed_segment, device, input_size=(1,3,224,224)):\n",
    "    inp = torch.rand(*input_size).to(device)\n",
    "    net.train()\n",
    "    set_segment_training(parsed_segment, train=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ori_output = net(inp)\n",
    "        parsed_graph_output = parsed_segment.forward(inp)\n",
    "\n",
    "    max_graph_err = torch.max(torch.abs(parsed_graph_output - ori_output))\n",
    "    if max_graph_err < 1e-05:\n",
    "        print('Parsed graph forward check passed')\n",
    "    else:\n",
    "        print('Parsed graph forward check failed: Max Difference {}'.format(max_graph_err))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def backward_check(net, parsed_segment, device, input_size=(1,3,224,224)):\n",
    "    inp = torch.rand(*input_size).to(device)\n",
    "    inp.requires_grad = True\n",
    "    net.train()\n",
    "    set_segment_training(parsed_segment, train=True)\n",
    "    ori_output = net(inp)\n",
    "    output_target = torch.rand(*ori_output.shape).to(device)\n",
    "    loss = torch.sum(output_target - ori_output)\n",
    "    loss.backward()\n",
    "    ori_grad = [p.grad.clone() for p in net.parameters()]\n",
    "    net.zero_grad()\n",
    "    del ori_output, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    parsed_graph_output = parsed_segment.forward(inp)\n",
    "    loss = torch.sum(output_target - parsed_graph_output)\n",
    "    loss.backward()\n",
    "    graph_grad = [p.grad.clone() for p in net.parameters()]\n",
    "\n",
    "    net.zero_grad()\n",
    "\n",
    "    max_graph_err = 0\n",
    "    for g1, g2 in zip(ori_grad, graph_grad):\n",
    "        if torch.norm(g1) > 1e-02:\n",
    "            rel_err = torch.max(torch.abs(g2 - g1)) / torch.norm(g1)\n",
    "        else:\n",
    "            rel_err = torch.max(torch.abs(g2 - g1))\n",
    "        if rel_err > max_graph_err:\n",
    "            max_graph_err = rel_err\n",
    "\n",
    "    if max_graph_err < 1e-03:\n",
    "        print('Parsed graph backward check passed')\n",
    "    else:\n",
    "        print('Parsed graph backward check failed: Max Difference {}'.format(max_graph_err))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "input_size = (4, 64, 32, 32)\n",
    "device = torch.device('cuda:0')\n",
    "net = CustomNet().to(device)\n",
    "net.train()\n",
    "inp = torch.rand(*input_size).to(device)\n",
    "G, source, target = net.parse_graph(inp)\n",
    "parsed_segment = Segment(G, source, target, do_checkpoint=False)\n",
    "forward_check(net, parsed_segment, device, input_size=input_size)\n",
    "backward_check(net, parsed_segment, device, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad_checkpoint_p36",
   "language": "python",
   "name": "grad_checkpoint_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
