{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from net.model_factory import model_factory\n",
    "from solver import ArbitrarySolver\n",
    "from graph import Segment\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and Specify Input size and cuda device\n",
    "Here we use darts_cifar10 model as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 108 36\n",
      "108 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 36\n",
      "144 144 72\n",
      "144 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 72\n",
      "288 288 144\n",
      "288 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n",
      "576 576 144\n"
     ]
    }
   ],
   "source": [
    "arch = 'darts_cifar10'\n",
    "device = 'cuda:0'\n",
    "input_size = (64, 3, 32, 32)\n",
    "model = model_factory[arch]().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Computation Graph of the Model\n",
    "Set the model to train status, create a random tensor of input size and send it into model.parse_graph function. The parse_graph function does a forward with input tensor and create the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "inp = torch.rand(*input_size).to(device)\n",
    "G, source, target = model.parse_graph(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve optimal checkpointing for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Division Tree\n",
      "Getting Max Terms\n",
      "Solving Optimal for Each Max Term\n"
     ]
    }
   ],
   "source": [
    "solver = ArbitrarySolver()\n",
    "run_graph, best_cost = solver.solve(G, source, target, use_tqdm=False)\n",
    "run_segment = Segment(run_graph, source, target, do_checkpoint=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CIFAR-10 dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=input_size[0],\n",
    "                                              shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=input_size[0],\n",
    "                                             shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up loss and optimizer, train and evaluate for two epochs\n",
    "The only difference with regular training is that we use run_segment to perform checkpointing training in training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.287\n",
      "[1,   200] loss: 2.131\n",
      "[1,   300] loss: 1.993\n",
      "[1,   400] loss: 1.849\n",
      "[1,   500] loss: 1.781\n",
      "[1,   600] loss: 1.744\n",
      "[1,   700] loss: 1.670\n",
      "[1] loss: 1.632\n",
      "[2,   100] loss: 1.623\n",
      "[2,   200] loss: 1.576\n",
      "[2,   300] loss: 1.533\n",
      "[2,   400] loss: 1.544\n",
      "[2,   500] loss: 1.505\n",
      "[2,   600] loss: 1.497\n",
      "[2,   700] loss: 1.471\n",
      "[2] loss: 1.414\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# train for 2 epoch and eval for 2 epoch\n",
    "for epoch in range(2):\n",
    "    # use model to switch between train and evaluation\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs.requires_grad = True\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # use run_segment to do checkpointing forward and backward for training\n",
    "        outputs = run_segment(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    eval_running_loss = 0.0\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # use model to do forward for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        eval_running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f' % (epoch + 1, eval_running_loss / len(testloader)))\n",
    "\n",
    "    # save model weights\n",
    "    torch.save(model.state_dict(), './checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad_checkpoint_p36",
   "language": "python",
   "name": "grad_checkpoint_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
